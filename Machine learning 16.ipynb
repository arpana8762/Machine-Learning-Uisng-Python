{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d3761dd-b4f5-4859-9b4c-2fa2de416036",
   "metadata": {},
   "source": [
    "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\r\n",
    "can they be mitigated?\r",
    "d how they work.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70956a25-52f3-482e-b039-a09630a7d72a",
   "metadata": {},
   "source": [
    "# Overfitting: \n",
    "\n",
    "Overfitting occurs when a model learns the training data too well, including its noise and outliers, which reduces its generalization ability to unseen data. The model performs excellently on the training data but poorly on test or validation data.\n",
    "\n",
    "# Consequences:\n",
    "- Poor performance on new/unseen data.\n",
    "- High variance.\n",
    "\n",
    "# Mitigation:\n",
    "- Use simpler models.\n",
    "- Introduce regularization techniques (L1, L2 regularization).\n",
    "- Increase the size of the training dataset.\n",
    "- Use dropout in neural networks.\n",
    "\n",
    "# Underfitting: \n",
    "Underfitting occurs when a model is too simple to capture the underlying structure of the data, leading to poor performance on both training and test data.\n",
    "\n",
    "# Consequences:\n",
    "- High bias.\n",
    "- Model fails to capture significant patterns in the data.\n",
    "# Mitigation:\n",
    "- Use more complex models.\n",
    "- Increase training duration or adjust hyperparameters.\n",
    "- Add more relevant features to the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b575a38-f2a3-4045-96f7-0d09b4da29a6",
   "metadata": {},
   "source": [
    "Q2: How can we reduce overfitting? Explain in brief.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799cc96d-2206-4eb4-b2c1-db3bd9539974",
   "metadata": {},
   "source": [
    "To reduce overfitting, the following strategies can be used:\n",
    "\n",
    "# Regularization:\n",
    "- L1 Regularization (Lasso): Adds a penalty proportional to the absolute value of coefficients.\n",
    "- L2 Regularization (Ridge): Adds a penalty proportional to the square of coefficients.\n",
    "\n",
    "# Pruning:\n",
    "- For decision trees, limit the depth or number of leaves.\n",
    "\n",
    "# Cross-Validation:\n",
    "- Use k-fold cross-validation to evaluate model generalization.\n",
    "\n",
    "# Dropout:\n",
    "- Randomly ignore a fraction of neurons during training in neural networks.\n",
    "\n",
    "# Early Stopping:\n",
    "- Stop training when performance on the validation set starts degrading.\n",
    "\n",
    "# Increase Training Data:\n",
    "- Collect more data or use data augmentation techniques.\n",
    "\n",
    "# Example: \n",
    "\n",
    "A neural network overfitting on a small image dataset can benefit from data augmentation (e.g., rotations, flips) and dropout layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f29bc9-e2e1-4332-b7d4-fb0a1375faa3",
   "metadata": {},
   "source": [
    "Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067bbb85-c684-4913-b230-5c2ca09f2333",
   "metadata": {},
   "source": [
    "# Definition: \n",
    "Underfitting occurs when a model cannot capture the underlying pattern of the data due to its simplicity or improper training.\n",
    "\n",
    "# Scenarios:\n",
    "\n",
    "- Insufficient Training: The model hasn‚Äôt been trained for enough iterations.\n",
    "- Model Complexity: Using a linear model on non-linear data.\n",
    "- Inadequate Features: Relevant features are missing or incorrectly engineered.\n",
    "- High Regularization: Too much regularization can overly constrain the model.\n",
    "- Small Model: Using a model with too few parameters.\n",
    "\n",
    "# Example: \n",
    "\n",
    "Training a linear regression model to predict housing prices on a dataset with complex patterns (non-linear relationships) will likely underfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85eaf3a-ae73-48d8-9df7-f9832fd3f2f5",
   "metadata": {},
   "source": [
    "\n",
    "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
    "variance, and how do they affect model performance?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb8f384-277a-4591-861d-159b6e5efd48",
   "metadata": {},
   "source": [
    "# Bias: \n",
    "\n",
    "Error due to simplifying assumptions in the model.\n",
    "\n",
    "- High bias leads to underfitting.\n",
    "\n",
    "Example: Using a linear model for non-linear data.\n",
    "\n",
    "# Variance: \n",
    "Error due to sensitivity to small fluctuations in the training data.\n",
    "\n",
    "- High variance leads to overfitting.\n",
    "Example: A deep neural network trained on a small dataset.\n",
    "\n",
    "# Bias-Variance Tradeoff:\n",
    "\n",
    "- Increasing model complexity reduces bias but increases variance.\n",
    "- Decreasing model complexity reduces variance but increases bias.\n",
    "\n",
    "# Impact on Performance:\n",
    "\n",
    "- High bias: Poor performance on both training and test data.\n",
    "- High variance: Good performance on training data but poor on test data.\n",
    "To balance bias and variance, select an appropriate model complexity using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4101f555-0cd5-4785-82ad-8b91c4e7caf7",
   "metadata": {},
   "source": [
    "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "How can you determine whether your model is overfitting or underfitting?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a5e639-f093-4e47-9e86-ce4e18570de1",
   "metadata": {},
   "source": [
    "# Methods for Detection:\n",
    "\n",
    "## Performance on Training and Test Data:\n",
    "\n",
    "- Overfitting: High training accuracy but low test accuracy.\n",
    "- Underfitting: Low accuracy on both training and test data.\n",
    "\n",
    "## Learning Curves:\n",
    "\n",
    "- Overfitting: Large gap between training and validation errors.\n",
    "- Underfitting: Both errors are high and converge.\n",
    "\n",
    "# Validation:\n",
    "\n",
    "Monitor model performance on a validation set.\n",
    "\n",
    "# Example:\n",
    "\n",
    "A decision tree achieving 100% accuracy on training data but only 70% on validation data indicates overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92ed2e3-671d-455f-840f-0b2787c99f09",
   "metadata": {},
   "source": [
    "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
    "and high variance models, and how do they differ in terms of their performance?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3a0c79-a858-4a10-aad4-81414a66442e",
   "metadata": {},
   "source": [
    "# Bias:\n",
    "\n",
    "- Simplistic model assumptions.\n",
    "- Leads to underfitting.\n",
    "\n",
    "Example: Linear Regression for complex data.\n",
    "\n",
    "# Performance:\n",
    "- Low on both training and test data.\n",
    "\n",
    "# Fix:\n",
    "- Increase model complexity.\n",
    "\n",
    "# Variance:\n",
    "- Model learns noise from training data.\n",
    "- Leads to overfitting.\n",
    "- Example: Decision Trees with no pruning.\n",
    "\n",
    "# Performance:\n",
    "- High on training data, low on test data.\n",
    "\n",
    "# Fix:\n",
    "- Use regularization or collect more data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37e04ca-678f-44e7-ac6d-671034d177c4",
   "metadata": {},
   "source": [
    "\n",
    "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
    "some common regularization techniques and how they work.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565a352b-6c47-4cf0-921c-915cf116c15d",
   "metadata": {},
   "source": [
    "# Definition: \n",
    "\n",
    "Regularization adds a penalty to the model‚Äôs loss function to discourage complex models.\n",
    "\n",
    "# Techniques:\n",
    "\n",
    "1. L1 Regularization (Lasso):\n",
    "- Adds the absolute value of coefficients as a penalty.\n",
    "- Shrinks some coefficients to zero (feature selection).\n",
    "- Formula: ùêøùëúùë†ùë†=MSE+Œª‚àë‚à£wi‚à£\n",
    "2. L2 Regularization (Ridge):\n",
    "- Adds the square of coefficients as a penalty.\n",
    "- Reduces magnitude but doesn‚Äôt set coefficients to zero.\n",
    "- Formula: ùêøùëúùë†ùë†=MSE+ùúÜ‚àëùë§ùëñ2\n",
    "  \n",
    "‚Äã3. Elastic Net:\n",
    "- Combines L1 and L2 regularization.\n",
    "4. Dropout:\n",
    "- Randomly drops neurons during training to prevent reliance on specific features.\n",
    "# Example:\n",
    "For a neural network prone to overfitting, adding L2 regularization or dropout can help maintain generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93362888-78bb-4ac9-bf5d-5634070fe641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
